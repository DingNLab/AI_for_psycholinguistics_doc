# å†…å®¹æ•´åˆ

## 0. åŠ è½½å·¥å…·åŒ…

* ä½¿ç”¨åˆ°çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…
    - **srilm**: è®¡ç®—é¢‘æ¬¡ã€è½¬ç§»æ¦‚ç‡ã€‚[[å®˜æ–¹æ–‡æ¡£]](https://srilm-python.readthedocs.io/en/latest/#)
    - **hanlp**: é€‚ç”¨äºåŸºæœ¬è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†è¯ã€è¯æ€§åˆ†æã€å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†æã€é™æ€è¯å‘é‡æå–ç­‰ç­‰ï¼Œå¯¹äºä¸­æ–‡æ¯”è¾ƒå‹å¥½ã€‚[[å®˜æ–¹æ–‡æ¡£]](https://hanlp.hankcs.com/docs/)
    - **Huggingfaceç³»åˆ—**: è°ƒç”¨å¼€æºæ·±åº¦å­¦ä¹ æ¨¡å‹å®Œæˆä»¥ä¸Šä¸¤è€…æåˆ°çš„ï¼Œä»¥åŠæ›´å¤šå…¶ä»–çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬ç”Ÿæˆç­‰ç­‰ã€‚[[å®˜ç½‘]](https://huggingface.co/)
* å…¶ä»–å¸¸ç”¨çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…
    - **nltk**ï¼šå¯ä»¥è°ƒç”¨ä¼—å¤šè¯­æ–™åº“ï¼ˆå¦‚wordnetç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥è¿›è¡Œä¸€ç³»åˆ—çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚[[å®˜æ–¹æ–‡æ¡£]](https://www.nltk.org/)
    - **spacy**ï¼šé€Ÿåº¦å¿«ã€åŠŸèƒ½å…¨é¢çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚[[å®˜æ–¹æ–‡æ¡£]](https://spacy.io/)
    - **stanza**ï¼šStanford CoreNLPçš„pythonç‰ˆæœ¬
    - **fastNLP**ï¼šå¤æ—¦å¤§å­¦åˆ¶ä½œçš„NLPå·¥å…·åŒ…


```python
# å¦‚æœåœ¨colabç­‰æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œå…ˆç”¨ä»¥ä¸‹å‘½ä»¤å»æ‰#å®‰è£…å·¥å…·åŒ…
#!pip install srilm
#!pip install hanlp
#!pip install transformers, tokenizers
```


```python
# å¦‚æœåªéœ€è¦æå–ä¸€éƒ¨åˆ†ç‰¹å¾ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°å¯¼å…¥ä»¥ä¸‹å·¥å…·åŒ…
import os
import re
import json
from collections import Counter, OrderedDict
from tqdm import tqdm

# æ•°æ®å¤„ç†åŠå¯è§†åŒ–
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib
matplotlib.rc("font", family='SimHei') # ç”¨æ¥æ˜¾ç¤ºä¸­æ–‡ï¼Œå¯¹äºmacosç³»ç»Ÿéœ€è¦æ¢ä¸€ä¸ªæ”¯æŒçš„å­—ä½“

# è‡ªç„¶è¯­è¨€å¤„ç†
from srilm import LM
import hanlp
import torch
from transformers import (
    BertTokenizer,
    GPT2LMHeadModel, 
    TextGenerationPipeline,
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    AutoModelForSeq2SeqLM,
    pipeline
    )

```

## 1. è¯æ±‡åŠè¯­ä¹‰ç‰¹å¾æå–

### 1.1 æ•°æ®é¢„å¤„ç†ï¼šåŠ è½½è¯­æ–™åº“ä»¥åŠè¿›è¡Œåˆ†è¯


```python
def filter_str(astr, tokenizer):
    '''
    # ä½¿ç”¨åˆ†è¯æ¨¡å‹æ¥åˆ†è¯
    è¾“å…¥: 
        astr: str, a sentence
        tokenizer: hanlp tokenizer
    è¾“å‡º:
        a sentence with words separated by space
    '''
    words = tokenizer(astr)
    return ' '.join(words)

def prepare_corpus(tokenizer, corpus, save_json_name):
    '''
    # å¯¹è¯­æ–™åº“è¿›è¡Œåˆ†è¯
    è¾“å…¥:
        tokenizer: hanlp tokenizer
        corpus: str, the path of corpus
        save_json_name: str, the path of saving json file
    è¾“å‡º: 
        
    '''
    with open(save_json_name, 'r', encoding='utf-8') as fp:
        wiki_texts = json.load(fp)
        wiki_texts_new = []
        for line in tqdm(wiki_texts):
            wiki_texts_new.append(filter_str(line, tokenizer))
        open(corpus, 'w').write('\n'.join(wiki_texts_new))

# åŠ è½½hanlpä¸­çš„åˆ†è¯æ¨¡å‹
hanlp_tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
# wikiè¯­æ–™
wiki_file = './srilm_data_model/wiki_demo/wiki_z.json'
# åˆ†è¯åè¯­æ–™æ–‡ä»¶
wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'
# æ‰§è¡Œ
prepare_corpus(hanlp_tok, wiki_file_tkd, wiki_file)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.35it/s]                 


### 1.2 åŸºäºè¯­æ–™åº“ç»Ÿè®¡çš„N-gramè®¡ç®—

#### 1.2.1 ä»è¯­æ–™åº“ä¸­ç”ŸæˆN-gramæ¨¡å‹
* å°†è¯­æ–™åº“ï¼ˆcorpusï¼‰å’ŒæŒ‡å®šçš„æ¨¡å‹è®¾ç½®ï¼ˆngramï¼‰è¾“å…¥æ¨¡å‹ï¼Œåœ¨æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼ˆmodel_pathï¼‰ä¸­è¾“å‡ºç»Ÿè®¡å¥½çš„æ¨¡å‹
* ç°æˆçš„N-gramè¯­æ–™åº“ï¼š[google n-gram](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)


```python
def generate_model(model_path, ngram, corpus):
    '''
    è¾“å…¥:
        model_path: str, ngramæ¨¡å‹çš„ä¿å­˜è·¯å¾„
        ngram: str, ngram-countè·¯å¾„
        corpus: str, corpusè·¯å¾„
    è¾“å‡º:
        
    '''
    cmd = '{} -text {} -order 3 -kndiscount3 -lm {}'.format(ngram, corpus, model_path)
    os.system(cmd)

ngram = '/home/zhang/acoustic_theory/workspace/21-12-30-srilm/srilm/bin/i686-m64/ngram-count'
wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'
model_path = './srilm_data_model/wiki_demo/wiki_z_word.lm'
generate_model(model_path, ngram, wiki_file_tkd)
```

    warning: discount coeff 1 is out of range: 0
    warning: discount coeff 7 is out of range: 1.91919



```python
model_path = './srilm_data_model/wiki/wiki_z_word.lm'
lm = LM(model_path, lower=True) # åŠ è½½N-gramæ¨¡å‹
```

#### 1.2.2 é‡‡ç”¨N-gramæ¨¡å‹è®¡ç®—è¯é¢‘
ç”¨srilmçš„LMæ¥è°ƒç”¨åˆšåˆšç”Ÿæˆçš„æ¨¡å‹ï¼Œé‡‡ç”¨`lm.logprob_strings(word, context)`æ¥ç”Ÿæˆ $\log{p \left( \rm{word} | context \right)}$ï¼Œwordæ˜¯å½“å‰å•è¯ï¼Œå½“contextæ˜¯ç©ºåˆ—è¡¨`[]`æ—¶ç›¸å½“äº1-gramå³è¯é¢‘


```python
# è®¡ç®—è¯é¢‘
print('*'*20 + ' è®¡ç®—è¯é¢‘ ' + '*'*20)
word_freq0_ = lm.logprob_strings('çš„', [])
word_freq1_ = lm.logprob_strings('è¥¿ç“œ', [])
word_freq2_ = lm.logprob_strings('æ¡Œå­', [])

# è¾“å‡ºç»“æœ
print('='*20 + 'P(çš„) vs P(è¥¿ç“œ) vs P(æ¡Œå­)' + '='*20)
print('P(çš„): ' + str(word_freq0_))
print('P(è¥¿ç“œ): ' + str(word_freq1_))
print('P(æ¡Œå­): ' + str(word_freq2_))
```

    ******************** è®¡ç®—è¯é¢‘ ********************
    ====================P(çš„) vs P(è¥¿ç“œ) vs P(æ¡Œå­)====================
    P(çš„): -1.3277089595794678
    P(è¥¿ç“œ): -5.5793938636779785
    P(æ¡Œå­): -5.5162577629089355


#### 1.2.3 é‡‡ç”¨N-gramæ¨¡å‹è®¡ç®—è½¬ç§»æ¦‚ç‡

å½“$n>1$æ—¶ï¼Œåœ¨`context`ä¸­æ”¾å…¥å‰$n-1$ä¸ªè¯ï¼Œé¡ºåºæ˜¯ä»å³åˆ°å·¦ã€‚


```python
tp1_ = lm.logprob_strings('è¥¿ç“œ', ['åƒ', 'å–œæ¬¢'])
tp2_ = lm.logprob_strings('æ¡Œå­', ['åƒ', 'å–œæ¬¢'])
print('='*10 + 'P(è¥¿ç“œ | åƒ, å–œæ¬¢) vs P(æ¡Œå­ | åƒ, å–œæ¬¢)' + '='*10)
print('P(è¥¿ç“œ | åƒ, å–œæ¬¢): ' + str(tp1_))
print('P(æ¡Œå­ | åƒ, å–œæ¬¢): ' + str(tp2_))
```

    ==========P(è¥¿ç“œ | åƒ, å–œæ¬¢) vs P(æ¡Œå­ | åƒ, å–œæ¬¢)==========
    P(è¥¿ç“œ | åƒ, å–œæ¬¢): -2.884925365447998
    P(æ¡Œå­ | åƒ, å–œæ¬¢): -6.211382865905762


#### 1.2.4 é‡‡ç”¨N-gramæ¨¡å‹è®¡ç®—surprisal
$\rm{surprisal} = -\log{ \it{p} \left( \rm{word} | context \right)}$ï¼Œæ‰€ä»¥åªè¦å–è´Ÿå³å¯ã€‚


```python
s1_ = -lm.logprob_strings('è¥¿ç“œ', ['åƒ', 'å–œæ¬¢'])
s2_ = -lm.logprob_strings('æ¡Œå­', ['åƒ', 'å–œæ¬¢'])
print('='*10 + 'surprisal(è¥¿ç“œ | åƒ, å–œæ¬¢) vs surprisal(æ¡Œå­ | åƒ, å–œæ¬¢)' + '='*10)
print('surprisal(è¥¿ç“œ | åƒ, å–œæ¬¢): ' + str(s1_))
print('surprisal(æ¡Œå­ | åƒ, å–œæ¬¢): ' + str(s2_))
```

    ==========surprisal(è¥¿ç“œ | åƒ, å–œæ¬¢) vs surprisal(æ¡Œå­ | åƒ, å–œæ¬¢)==========
    surprisal(è¥¿ç“œ | åƒ, å–œæ¬¢): 2.884925365447998
    surprisal(æ¡Œå­ | åƒ, å–œæ¬¢): 6.211382865905762


#### 1.2.5 é‡‡ç”¨N-gramæ¨¡å‹è®¡ç®—entropy
$\rm{entropy} = \sum \left( p*surprisal \right)$ï¼Œæ‰€ä»¥å¯¹äºç»™å®šçš„contextï¼Œå¯¹æ‰€æœ‰çš„è¯æ¥è®¡ç®—surprisalç„¶åæ±‚æœŸæœ›


```python
model_path = './srilm_data_model/wiki/wiki_z_morpheme.lm'
lm = LM(model_path, lower=True) # åŠ è½½N-gramæ¨¡å‹
def entropy_cal(lm, context):
    # entropy
    raw_text_idx = [lm.vocab.intern(w) for w in context]
    vocab_num = lm.vocab.max_interned() + 1
    logprobs = [lm.logprob(i, raw_text_idx) for i in range(vocab_num)]
    logprobs_np = np.array(logprobs)
    logprobs_np_ = logprobs_np[logprobs_np > -np.inf]
    entropy_ = sum(-np.power(10, logprobs_np_)*logprobs_np_)
    return entropy_

print('='*10 + 'entropy(è´) vs entropy(ã€‚)' + '='*10)
e1_ = entropy_cal(lm, ['è´'])
print('entropy(è´): ' + str(e1_))
e2_ = entropy_cal(lm, ['ã€‚'])
print('entropy(ã€‚): ' + str(e2_))
```

    ==========entropy(è´) vs entropy(ã€‚)==========
    entropy(è´): 0.03182660213747036
    entropy(ã€‚): 2.5136258206385347


### 1.3 åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è½¬ç§»æ¦‚ç‡è®¡ç®—

ä»¥gpt-2ä¸ºä¾‹ï¼Œé‡‡ç”¨çš„æ¨¡å‹ä¸º[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)

#### 1.3.1 åŠ è½½æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆ†è¯æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹



```python
from transformers import BertTokenizer, GPT2LMHeadModel
ckpt_path = "uer/gpt2-chinese-cluecorpussmall" # checkpointæ¨¡å‹è·¯å¾„
tokenizer = BertTokenizer.from_pretrained(ckpt_path) # åˆ†è¯å™¨
model = GPT2LMHeadModel.from_pretrained(ckpt_path) # è¯­è¨€æ¨¡å‹
```

#### 1.3.2 è·å–æ¨¡å‹çš„è½¬ç§»æ¦‚ç‡
åç»­çš„surprisalå’Œentropyä¹Ÿå¯ä»¥é€šè¿‡è½¬ç§»æ¦‚ç‡ç®—å‡ºæ¥ï¼Œä¸1.2éƒ¨åˆ†ç±»ä¼¼


```python
model.config.output_hidden_states = True  # åœ¨æ¨¡å‹è®¾ç½®configä¸­è®¾ç½®ä¸ºTrueï¼Œå¯ä»¥è®©æ¨¡å‹è¾“å‡ºhidden states
inputs = tokenizer('å°æ˜å–œæ¬¢åƒè¥¿ç“œã€‚å°æ˜å–œæ¬¢æ‰“ç¯®çƒã€‚å°æ˜ç»å¸¸å»èŠ±åº—', return_tensors="pt") # å¯¹å¥å­è¿›è¡Œåˆ†è¯
outputs = model(**inputs)  # å°†åˆ†è¯åçš„å¥å­è¾“å…¥æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹è¾“å‡ºçš„ç»“æœ

print('='*10 + 'è¾“å…¥å­—æ•°: ' + '='*10)
print(len(inputs['input_ids'][0]))

print('='*10 + 'è½¬ç§»æ¦‚ç‡ç»´åº¦: ' + '='*10)
print(str(outputs.logits[0].shape) + '  è¾“å…¥å­—æ•° x æ€»å­—æ•°')
```

    ==========è¾“å…¥å­—æ•°: ==========
    25
    ==========è½¬ç§»æ¦‚ç‡ç»´åº¦: ==========
    torch.Size([25, 21128])  è¾“å…¥å­—æ•° x æ€»å­—æ•°


### 1.4 è¯æ€§


```python
## 0. åˆ†è¯
sent_ex = 'è¿™ä¸ªé—¨è¢«é”äº†ï¼Œé”å¾ˆéš¾è¢«æ‰“å¼€ã€‚'
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
tks = tok(sent_ex)
print('0. åˆ†è¯ç»“æœï¼š')
print(tks)

## 1. è¯æ€§æ ‡æ³¨
pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)
print('1. è¯æ€§æ ‡æ³¨ï¼š')
print(pos(tks))
```

    Building model [5m[33m...[0m[0m          

    0. åˆ†è¯ç»“æœï¼š
    ['è¿™ä¸ª', 'é—¨', 'è¢«', 'é”', 'äº†', 'ï¼Œ', 'é”', 'å¾ˆéš¾', 'è¢«', 'æ‰“å¼€', 'ã€‚']


                                                 

    1. è¯æ€§æ ‡æ³¨ï¼š
    ['DT', 'NN', 'SB', 'VV', 'SP', 'PU', 'VV', 'AD', 'SB', 'VV', 'PU']


### 1.5 è¯å‘é‡

#### 1.5.1 è·å–é™æ€è¯å‘é‡ï¼šä»¥word2vecä¸ºä¾‹
* hanlpæ”¯æŒè°ƒç”¨å„ç§é™æ€è¯å‘é‡ï¼Œ åŒ…æ‹¬[word2vec](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/word2vec.html), [glove](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/glove.html)ç­‰ç­‰ï¼Œå…·ä½“çš„æ¨¡å‹åŠæ–‡çŒ®å¯ä»¥åœ¨é“¾æ¥æ–‡æ¡£ä¸­è¿›è¡Œé€‰æ‹©ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ç»´åº¦è¶Šé«˜è¶Šå‡†ç¡®ã€‚


```python
word2vec = hanlp.load(hanlp.pretrained.word2vec.MERGE_SGNS_BIGRAM_CHAR_300_ZH) # åŠ è½½word2vecè¯å‘é‡
word2vec('ä¸­å›½')
```

                                                            




    tensor([ 1.4234e-02,  8.3600e-02,  2.4145e-02, -1.0256e-01, -1.0829e-01,
            -2.6786e-02, -9.6481e-02,  9.0537e-02, -5.4941e-02,  4.5936e-02,
            -4.2577e-02, -5.1776e-02,  4.9661e-02, -3.2703e-02, -6.6407e-03,
             9.8313e-03,  4.2377e-02, -7.1969e-02,  6.7363e-02, -1.2679e-01,
             1.3423e-03,  1.8129e-02,  1.3923e-02,  6.0298e-02,  2.9974e-02,
             3.4969e-02,  4.7053e-02, -1.4874e-02,  6.6235e-02, -1.5579e-01,
            -1.1716e-01,  8.8726e-02,  6.0976e-02, -8.0692e-02, -3.1017e-02,
            -1.3132e-02,  5.4841e-02,  4.0733e-02, -1.5295e-01, -7.8516e-02,
             6.6119e-02,  2.9393e-02, -3.0162e-02, -4.3704e-02,  8.3047e-03,
            -7.7654e-02, -1.5644e-02,  6.2678e-02,  7.3149e-02, -1.9128e-02,
             2.7543e-02, -1.4893e-02, -1.2223e-02,  9.6474e-02,  2.1985e-02,
             4.4640e-02, -2.4626e-02,  9.8536e-02, -1.3777e-01,  5.1621e-02,
             9.5042e-02, -3.2784e-02,  2.8697e-02, -1.3267e-02,  1.1536e-02,
            -9.0047e-02, -7.2654e-02, -8.7082e-04, -3.6991e-02,  1.6448e-03,
             2.6809e-02, -7.5198e-02, -2.6094e-02,  6.5516e-03, -7.2922e-02,
            -6.3720e-02, -6.4798e-03,  1.3006e-02,  1.7040e-02, -4.3527e-02,
             1.6448e-03, -4.0217e-02,  2.1293e-02, -4.1442e-02, -4.9964e-02,
             1.0784e-02,  1.2986e-01, -1.7174e-02,  9.0332e-02,  8.1890e-04,
            -4.3150e-02, -6.7029e-02, -4.6127e-02, -6.4486e-02, -1.8022e-02,
             1.3425e-02,  6.9962e-02, -1.4400e-02,  6.0225e-03, -3.7480e-03,
             8.5195e-03, -2.2870e-02, -4.1049e-02, -1.8603e-02, -5.3075e-02,
            -7.1510e-02,  9.2589e-03, -6.3029e-03, -2.4524e-02, -3.4340e-02,
            -8.8730e-02,  1.5332e-02,  2.8820e-02,  1.8295e-02, -5.8320e-02,
            -2.7167e-02, -1.7402e-02, -7.7428e-02, -1.0769e-01, -1.0446e-01,
             4.5363e-02, -6.3230e-02,  8.3784e-02,  5.3965e-02,  2.0121e-02,
            -3.7716e-02, -2.0752e-02, -6.2321e-02, -1.3778e-01,  5.0385e-02,
             8.9087e-06, -8.1429e-02,  6.1611e-02, -4.1132e-02,  7.4521e-02,
            -5.0390e-02, -1.6549e-02,  4.1053e-02, -1.7056e-02, -1.2268e-02,
            -1.3683e-02,  1.0725e-02, -5.9534e-02, -3.3246e-02,  3.8279e-02,
            -3.6564e-02,  6.8516e-02,  6.6845e-02,  4.3522e-02, -2.3375e-02,
            -1.3111e-02,  1.4433e-03,  3.9912e-02,  3.8543e-03,  8.9713e-02,
             1.9988e-02,  9.5058e-04, -7.2403e-02, -3.7107e-02, -6.4932e-02,
            -2.1959e-02,  3.4034e-02, -2.9596e-02, -6.8593e-02, -1.9584e-02,
             4.0717e-02, -1.0285e-01, -6.5889e-03,  9.2453e-03, -4.2289e-02,
            -5.7992e-02,  3.3845e-02,  1.3048e-02, -5.1361e-02,  7.8392e-02,
            -1.9344e-02, -1.0448e-01,  4.1529e-02, -9.7657e-02, -3.4509e-03,
             4.9083e-02,  5.5863e-02,  8.7877e-03, -1.1969e-01,  7.1582e-02,
             2.4624e-02, -2.8234e-03, -1.0275e-01, -8.0798e-02, -1.2945e-01,
             1.7228e-02, -8.7083e-02, -4.5541e-02, -3.6977e-02,  7.5634e-02,
             6.3264e-02, -1.0102e-01, -9.6761e-02, -1.7960e-02, -1.6474e-02,
             6.5089e-02, -5.6679e-02,  1.7903e-02, -6.3342e-02,  2.1894e-02,
            -8.5694e-03, -2.0418e-02,  9.6943e-02,  6.6336e-02,  5.3024e-02,
             7.7205e-02,  7.5687e-02, -2.4854e-02, -8.4196e-02,  7.2153e-02,
            -3.3994e-02,  2.7743e-02,  7.6132e-02,  1.2271e-01,  8.2420e-02,
             2.2781e-02,  6.0472e-03, -1.5400e-01, -1.1090e-01, -1.8680e-03,
             9.7762e-02,  3.7373e-03, -2.6415e-02,  1.7530e-02,  9.8943e-03,
            -4.3207e-02,  4.6805e-02,  1.3863e-02, -5.2318e-02, -3.4550e-03,
            -3.7918e-02,  2.9433e-02,  3.3142e-02,  8.7807e-03,  3.0049e-02,
             8.8094e-02,  1.4916e-03, -1.7431e-02, -2.5317e-02, -1.6277e-02,
             1.1268e-02,  9.4293e-02,  3.3744e-02, -3.4135e-02,  6.1734e-04,
            -5.8349e-02,  1.2800e-01,  2.4264e-03, -1.0573e-01, -2.0444e-02,
             3.9112e-02, -1.4461e-01,  6.4038e-02, -8.3256e-03, -4.6320e-02,
            -1.3400e-02,  1.2040e-02,  7.3522e-02, -1.6663e-02, -1.2628e-03,
            -2.7094e-02, -1.8414e-03,  6.0205e-02, -6.7361e-02,  5.6380e-02,
             2.3484e-03, -4.5203e-03,  4.1993e-02,  2.9977e-02, -1.2228e-02,
             2.8904e-03, -1.7870e-02, -1.3307e-02, -4.5424e-02, -3.1245e-02,
             4.0651e-03,  1.0091e-01,  6.3333e-02,  1.5903e-01,  9.9152e-02,
            -2.0661e-02,  6.4784e-03,  1.3163e-03,  2.6181e-02, -9.9187e-03,
             1.4386e-02, -4.5888e-02,  5.6548e-02,  3.5045e-02,  5.5262e-02,
             3.0622e-02,  9.1758e-03, -1.0747e-01,  5.5859e-03, -5.0639e-02],
           device='cuda:1')



- æ•è·äº†æ€§åˆ«ä¿¡æ¯
- æ•è·äº†é¦–éƒ½ä¿¡æ¯


```python
print(torch.nn.functional.cosine_similarity(
    word2vec('å›½ç‹')-word2vec('ç‹å¦ƒ'), 
    word2vec('ç”·')-word2vec('å¥³'), dim=0)
      )
print(torch.nn.functional.cosine_similarity(
    word2vec('å…¬ä¸»')-word2vec('ç‹å¦ƒ'), 
    word2vec('ç”·')-word2vec('å¥³'), dim=0)
      )
```

    tensor(0.1429, device='cuda:1')
    tensor(0.0366, device='cuda:1')



```python
print(torch.nn.functional.cosine_similarity(
    word2vec('æ—¥æœ¬')-word2vec('ä¸œäº¬'), 
    word2vec('ä¸­å›½')-word2vec('åŒ—äº¬'), dim=0)
      )
print(torch.nn.functional.cosine_similarity(
    word2vec('éŸ©å›½')-word2vec('ä¸œäº¬'), 
    word2vec('ä¸­å›½')-word2vec('åŒ—äº¬'), dim=0)
      )

```

    tensor(0.4674, device='cuda:1')
    tensor(0.3933, device='cuda:1')


- è®¡ç®—ç›¸ä¼¼è¯


```python
# å•ä¸ªè¯
print(word2vec.most_similar('åŒ—äº¬')) 
print('\n')
```

    {'ä¸Šæµ·': 0.6443496942520142, 'å¤©æ´¥': 0.6384099721908569, 'è¥¿å®‰': 0.611718475818634, 'å—äº¬': 0.6113559603691101, 'åŒ—äº¬å¸‚': 0.6093109846115112, 'æµ·æ·€': 0.6049214601516724, 'å¹¿å·': 0.5977935791015625, 'äº¬åŸ': 0.5955069661140442, 'æ²ˆé˜³': 0.5865166187286377, 'æ·±åœ³': 0.580772876739502}
    
    


#### 1.5.2 è·å–åŸºäºä¸Šä¸‹æ–‡çš„è¯å‘é‡ï¼šè¯­è¨€æ¨¡å‹çš„éšè—å±‚è¡¨å¾

åŒæ ·ä»¥1.3ä¸­è°ƒç”¨çš„[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)ä¸ºä¾‹


```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
ckpt_path = "uer/gpt2-chinese-cluecorpussmall" # checkpointæ¨¡å‹è·¯å¾„
tokenizer = BertTokenizer.from_pretrained(ckpt_path) # åˆ†è¯å™¨
model = GPT2LMHeadModel.from_pretrained(ckpt_path) # è¯­è¨€æ¨¡å‹
```


```python
model.config.output_hidden_states = True
inputs = tokenizer('å°æ˜å–œæ¬¢åƒè¥¿ç“œã€‚å°æ˜å–œæ¬¢æ‰“ç¯®çƒã€‚å°æ˜ç»å¸¸å»èŠ±åº—', return_tensors="pt")
outputs = model(**inputs)

print('\n' + '='*10 + 'æœ€åä¸€å±‚è¾“å‡ºçš„å†…éšè¡¨å¾ç»´åº¦: ' + '='*10)
print(str(outputs.hidden_states[-1].shape) + '  1 x è¾“å…¥å­—æ•° x è¡¨å¾ç»´åº¦')
```

    
    ==========æœ€åä¸€å±‚è¾“å‡ºçš„å†…éšè¡¨å¾ç»´åº¦: ==========
    torch.Size([1, 25, 768])  1 x è¾“å…¥å­—æ•° x è¡¨å¾ç»´åº¦


## 2. å¥æ³•ç‰¹å¾æå–

### 2.1 å¥æ³•ç‰¹å¾æŠ½å–


```python
import hanlp
```


```python
## workshopä¸­çš„ä¾‹å­ï¼Œç ”ç©¶ä¸­ä¸€èˆ¬ä¼šæŠŠæ ‡ç‚¹å»æ‰ï¼Œä½†æ˜¯è¿™é‡Œä¿ç•™äº†æ ‡ç‚¹ï¼Œæ¨¡å‹ä¹Ÿæ˜¯èƒ½å¤Ÿè§£ææ ‡ç‚¹çš„
Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
doc = Hanlp('æ¬¢è¿å¤§å®¶å‚åŠ å·¥ä½œåŠï¼', tasks=['dep', 'con']) # åœ¨tasksä¸­é€‰æ‹©éœ€è¦çš„ä»»åŠ¡ï¼Œå¦‚æœä¸è®¾ç½®å°±è¿›è¡Œæ‰€æœ‰ä»»åŠ¡ï¼ˆè¿è¡Œèµ·æ¥ä¼šæ…¢ä¸€ç‚¹ï¼‰
doc.pretty_print()
```

                                                 


<div style="display: table; padding-bottom: 1rem;"><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Dep&nbsp;Tre&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>â”Œâ”¬â”€â”€â”¬â”€â”€&nbsp;<br>â”‚â”‚&nbsp;&nbsp;â””â”€â–º&nbsp;<br>â”‚â””â”€â–ºâ”Œâ”€â”€&nbsp;<br>â”‚&nbsp;&nbsp;&nbsp;â””â”€â–º&nbsp;<br>â””â”€â”€â”€â”€â”€â–º&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Relat&nbsp;<br>â”€â”€â”€â”€â”€&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">P&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>_â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºNPâ”€â”€â”€â”€â”¤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºVP&nbsp;â”€â”€â”€â”&nbsp;&nbsp;&nbsp;<br>_â”€â”€â”€â–ºNP&nbsp;â”€â”€â”€â”´â–ºVP&nbsp;â”€â”€â”€â”€â–ºIP&nbsp;â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºIP<br>_â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;</pre></div>


#### 2.1.1 æˆåˆ†å¥æ³•

æˆåˆ†å¥æ³•è¾“å‡ºå¾—åˆ°çš„æ˜¯ä¸€ä¸ªæ ‘ç»“æ„çš„æ•°æ®ï¼Œå¯ä»¥çœ‹ä½œä¸€ä¸ªåµŒå¥—çš„åˆ—è¡¨ã€‚æˆ‘ä»¬å¯ä»¥ï¼š
* è®¿é—®å¥æ³•æ ‘çš„ä¸€äº›å±æ€§
* è½¬æ¢ä¸ºæ‹¬å·è¡¨ç¤ºæ³•ï¼Œè®¡ç®—æ‹¬å·æ•°é‡
* è®¿é—®å¥æ³•æ ‘çš„å­æ ‘


```python
Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
doc = Hanlp('æ¬¢è¿å¤§å®¶å‚åŠ å·¥ä½œåŠï¼')
tree = doc['con']
```

                                                 


```python
# å¶ç»“ç‚¹çš„ä½ç½®
for i in range(len(tree.leaves())):
    print(tree.leaf_treeposition(i))
```

    (0, 0, 0, 0)
    (0, 0, 1, 0, 0)
    (0, 0, 2, 0, 0, 0)
    (0, 0, 2, 0, 1, 0, 0)
    (0, 1, 0)



```python
tree[0, 0, 1, 0, 0]
```




    'å¤§å®¶'




```python
# è½¬ä¸ºæ‹¬å·è¡¨ç¤ºæ³•
bracket_form = tree.pformat().replace ('\n', '').replace(' ', '') # å»æ‰æ¢è¡Œå’Œç©ºæ ¼
bracket_form
```




    '(TOP(IP(VP(VVæ¬¢è¿)(NP(PNå¤§å®¶))(IP(VP(VVå‚åŠ )(NP(NNå·¥ä½œåŠ)))))(PUï¼)))'




```python
# è½¬æ¢ä¸ºChomsky Normal Formï¼Œå¯ä»¥ç”¨tree.un_chomsky_normal_form()è½¬æ¢å›æ¥
tree.chomsky_normal_form() 
bracket_form = tree.pformat().replace ('\n', '').replace(' ', '')
print(bracket_form)
```

    (TOP(IP(VP(VVæ¬¢è¿)(VP|<NP-IP>(NP(PNå¤§å®¶))(IP(VP(VVå‚åŠ )(NP(NNå·¥ä½œåŠ))))))(PUï¼)))



```python
# è¾“å‡ºä¸­æœ‰äº›èŠ‚ç‚¹åªæ´¾ç”Ÿå‡ºä¸€æ”¯ï¼Œæ˜¯å†—ä½™çš„ï¼ˆä¾‹å¦‚æœ€å¤–å±‚çš„TOPæ ¹ç»“ç‚¹åªæ´¾ç”Ÿå‡ºIPï¼Œä»¥åŠå¥å­ä¸­çš„IPåªæ´¾ç”Ÿå‡ºVPï¼‰ï¼Œå¯ä»¥é€‰æ‹©å‹ç¼©èŠ‚ç‚¹
tree.collapse_unary(collapseRoot=True, joinChar='|') # å‹ç¼©å†—ä½™èŠ‚ç‚¹ï¼Œå‹ç¼©çš„èŠ‚ç‚¹ç”¨ï½œæ¥è¡¨ç¤º
bracket_form = tree.pformat().replace ('\n', '').replace(' ', '')
bracket_form 
```




    '(TOP|IP(VP(VVæ¬¢è¿)(VP|<NP-IP>(NP(PNå¤§å®¶))(IP|VP(VVå‚åŠ )(NP(NNå·¥ä½œåŠ)))))(PUï¼))'




```python
import re
import pandas as pd
# è®¡ç®—æ‹¬å·è¡¨ç¤ºæ³•ä¸­æ¯ä¸ªè¯çš„æ‹¬å·æ•°
bracket_clean= re.sub("([^()])", "", bracket_form) # åªä¿ç•™æ‹¬å·
print(bracket_clean)

# è®¡ç®—å·¦æ‹¬å·æ•°
left_bracket = [len(re.findall("\(", i)) for i in bracket_clean] 
left_bracket_count = []
for i in left_bracket:
    if len(left_bracket_count) == 0 or (i == 1 and j != 1):
        left_bracket_count.append(1)
    elif i == 1 and j == 1:
        left_bracket_count[-1] += 1
    j = i
print("å·¦æ‹¬å·æ•°:", left_bracket_count)

# è®¡ç®—å³æ‹¬å·æ•°
right_bracket = [len(re.findall("\)", i)) for i in bracket_clean] 
right_bracket_count = []; j = 0
for i in right_bracket:
    if i == 1 and j != 1:
        right_bracket_count.append(1)
    elif i == 1 and j == 1:
        right_bracket_count[-1] += 1
    j = i
print("å³æ‹¬å·æ•°:", right_bracket_count)

# å¯ä»¥ä¿å­˜ä¸º dataframe è¿›è¡Œè¿›ä¸€æ­¥çš„å¥æ³•ç‰¹å¾åˆ†æ
df_bracket = pd.DataFrame([tree.leaves(), left_bracket_count, right_bracket_count]).T
df_bracket.columns = ['word', 'left_bracket', 'right_bracket']
# df_bracket.to_csv('bracket.csv', index=False) # ä¿å­˜ä¸ºcsvæ–‡ä»¶
df_bracket
```

    ((()((())(()(()))))())
    å·¦æ‹¬å·æ•°: [3, 3, 2, 2, 1]
    å³æ‹¬å·æ•°: [1, 2, 1, 5, 2]





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>left_bracket</th>
      <th>right_bracket</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>æ¬¢è¿</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>å¤§å®¶</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>å‚åŠ </td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>å·¥ä½œåŠ</td>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ï¼</td>
      <td>1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




```python
# å¥æ³•æ ‘çš„å±æ€§
print("Terminal nodes:", tree.leaves())
print("Tree depth:", tree.height())
print("Tree productions:", tree.productions())
print("Part of Speech:", tree.pos())
```

    Terminal nodes: ['æ¬¢è¿', 'å¤§å®¶', 'å‚åŠ ', 'å·¥ä½œåŠ', 'ï¼']
    Tree depth: 7
    Tree productions: [TOP|IP -> VP PU, VP -> VV VP|<NP-IP>, VV -> 'æ¬¢è¿', VP|<NP-IP> -> NP IP|VP, NP -> PN, PN -> 'å¤§å®¶', IP|VP -> VV NP, VV -> 'å‚åŠ ', NP -> NN, NN -> 'å·¥ä½œåŠ', PU -> 'ï¼']
    Part of Speech: [('æ¬¢è¿', 'VV'), ('å¤§å®¶', 'PN'), ('å‚åŠ ', 'VV'), ('å·¥ä½œåŠ', 'NN'), ('ï¼', 'PU')]



```python
# å¥æ³•æ ‘çš„åµŒå¥—ç»“æ„
for i in tree.subtrees():  # æ ¹æ®Tree productionsï¼Œéå†æ‰€æœ‰çš„å­æ ‘ï¼Œæ¯ä¸€æ£µå­æ ‘éƒ½æ˜¯ä¸€ä¸ªTreeå¯¹è±¡ï¼Œå¯ä»¥è¿›è¡Œä¹‹å‰ç›¸åŒçš„æ“ä½œ
    print(i)
```

    (TOP|IP
      (VP
        (VV æ¬¢è¿)
        (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))))
      (PU ï¼))
    (VP (VV æ¬¢è¿) (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))))
    (VV æ¬¢è¿)
    (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ))))
    (NP (PN å¤§å®¶))
    (PN å¤§å®¶)
    (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))
    (VV å‚åŠ )
    (NP (NN å·¥ä½œåŠ))
    (NN å·¥ä½œåŠ)
    (PU ï¼)



```python
# é€šè¿‡ç´¢å¼•è®¿é—®å¥æ³•æ ‘çš„å­æ ‘
treepositions = tree.treepositions() # æ‰€æœ‰èŠ‚ç‚¹çš„ç´¢å¼•
treepositions
```




    [(),
     (0,),
     (0, 0),
     (0, 0, 0),
     (0, 1),
     (0, 1, 0),
     (0, 1, 0, 0),
     (0, 1, 0, 0, 0),
     (0, 1, 1),
     (0, 1, 1, 0),
     (0, 1, 1, 0, 0),
     (0, 1, 1, 1),
     (0, 1, 1, 1, 0),
     (0, 1, 1, 1, 0, 0),
     (1,),
     (1, 0)]




```python
for i in treepositions: # éå†æ‰€æœ‰èŠ‚ç‚¹
    print(tree[i])
```

    (TOP|IP
      (VP
        (VV æ¬¢è¿)
        (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))))
      (PU ï¼))
    (VP (VV æ¬¢è¿) (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))))
    (VV æ¬¢è¿)
    æ¬¢è¿
    (VP|<NP-IP> (NP (PN å¤§å®¶)) (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ))))
    (NP (PN å¤§å®¶))
    (PN å¤§å®¶)
    å¤§å®¶
    (IP|VP (VV å‚åŠ ) (NP (NN å·¥ä½œåŠ)))
    (VV å‚åŠ )
    å‚åŠ 
    (NP (NN å·¥ä½œåŠ))
    (NN å·¥ä½œåŠ)
    å·¥ä½œåŠ
    (PU ï¼)
    ï¼


#### 2.1.2 ä¾å­˜å¥æ³•
* ä¾å­˜å¥æ³•çš„æ•°æ®ç»“æ„æ›´åŠ ç®€å•ï¼Œä¸ºä¸€ä¸ªåˆ—è¡¨`[(head, relation), ... ]`ã€‚åˆ—è¡¨ä¸­ç¬¬$i$ä¸ªå€¼ä¸­åŒ…æ‹¬äº†å®ƒçš„æ ¸å¿ƒè¯çš„ä½ç½®ä»¥åŠå®ƒä¸æ ¸å¿ƒè¯ä¹‹é—´çš„ä¾å­˜å…³ç³»


```python
Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
doc = Hanlp('æ¬¢è¿å¤§å®¶å‚åŠ å·¥ä½œåŠï¼')
doc['dep']
```

                                                 




    [(0, 'root'), (1, 'dobj'), (1, 'dep'), (3, 'dobj'), (1, 'punct')]




```python
# å¯ä»¥ä¿å­˜ä¸º dataframe è¿›è¡Œè¿›ä¸€æ­¥çš„å¥æ³•ç‰¹å¾åˆ†æ
df_dep = pd.DataFrame(doc['dep'], columns=['head', 'rel'])
df_dep['word'] = doc['tok/fine']
df_dep = df_dep[['word', 'head', 'rel']]
df_dep
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>head</th>
      <th>rel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>æ¬¢è¿</td>
      <td>0</td>
      <td>root</td>
    </tr>
    <tr>
      <th>1</th>
      <td>å¤§å®¶</td>
      <td>1</td>
      <td>dobj</td>
    </tr>
    <tr>
      <th>2</th>
      <td>å‚åŠ </td>
      <td>1</td>
      <td>dep</td>
    </tr>
    <tr>
      <th>3</th>
      <td>å·¥ä½œåŠ</td>
      <td>3</td>
      <td>dobj</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ï¼</td>
      <td>1</td>
      <td>punct</td>
    </tr>
  </tbody>
</table>
</div>



### 2.2 æ‰¹é‡æ“ä½œ

åªéœ€è¦å°†è¦å¤„ç†çš„å¥å­æ”¾åœ¨listä¸­ï¼Œä¸€èµ·è¿›è¡Œç‰¹å¾æŠ½å–å³å¯ã€‚è¿™å¯¹æ‰€æœ‰ç‰¹å¾éƒ½é€‚ç”¨ï¼Œä¸ä»…æ˜¯å¥æ³•ç‰¹å¾ã€‚


```python
sentences = ['2023å¹´å¿ƒç†è¯­è¨€å­¦ä¼šåœ¨å¹¿å·å¬å¼€ã€‚', 'æ¬¢è¿å¤§å®¶å‚åŠ å·¥ä½œåŠï¼']
docs = Hanlp(sentences)
docs.pretty_print()
```


<div style="display: table; padding-bottom: 1rem;"><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Dep&nbsp;Tree&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”Œâ”€â–º&nbsp;<br>â”Œâ”€â”€â”€â–ºâ””â”€â”€&nbsp;<br>â”‚&nbsp;&nbsp;&nbsp;â”Œâ”€â”€â–º&nbsp;<br>â”‚&nbsp;&nbsp;&nbsp;â”‚â”Œâ”€â–º&nbsp;<br>â”‚â”Œâ”€â–ºâ””â”´â”€â”€&nbsp;<br>â”‚â”‚â”Œâ”€â–ºâ”Œâ”€â”€&nbsp;<br>â”‚â”‚â”‚&nbsp;&nbsp;â””â”€â–º&nbsp;<br>â””â”´â”´â”€â”€â”¬â”€â”€&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â””â”€â–º&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Toke&nbsp;<br>â”€â”€â”€â”€&nbsp;<br>2023&nbsp;<br>å¹´&nbsp;&nbsp;&nbsp;&nbsp;<br>å¿ƒç†&nbsp;&nbsp;&nbsp;<br>è¯­è¨€&nbsp;&nbsp;&nbsp;<br>å­¦ä¼š&nbsp;&nbsp;&nbsp;<br>åœ¨&nbsp;&nbsp;&nbsp;&nbsp;<br>å¹¿å·&nbsp;&nbsp;&nbsp;<br>å¬å¼€&nbsp;&nbsp;&nbsp;<br>ã€‚&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Relati&nbsp;<br>â”€â”€â”€â”€â”€â”€&nbsp;<br>nummod&nbsp;<br>nsubj&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nsubj&nbsp;&nbsp;<br>prep&nbsp;&nbsp;&nbsp;<br>pobj&nbsp;&nbsp;&nbsp;<br>root&nbsp;&nbsp;&nbsp;<br>punct&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Po&nbsp;<br>â”€â”€&nbsp;<br>NT&nbsp;<br>M&nbsp;&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>P&nbsp;&nbsp;<br>NR&nbsp;<br>VV&nbsp;<br>PU&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Toke&nbsp;<br>â”€â”€â”€â”€&nbsp;<br>2023&nbsp;<br>å¹´&nbsp;&nbsp;&nbsp;&nbsp;<br>å¿ƒç†&nbsp;&nbsp;&nbsp;<br>è¯­è¨€&nbsp;&nbsp;&nbsp;<br>å­¦ä¼š&nbsp;&nbsp;&nbsp;<br>åœ¨&nbsp;&nbsp;&nbsp;&nbsp;<br>å¹¿å·&nbsp;&nbsp;&nbsp;<br>å¬å¼€&nbsp;&nbsp;&nbsp;<br>ã€‚&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">NER&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>â”€â”€â”€â–ºDATE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â”€â”€â”€â–ºLOCATION&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Toke&nbsp;<br>â”€â”€â”€â”€&nbsp;<br>2023&nbsp;<br>å¹´&nbsp;&nbsp;&nbsp;&nbsp;<br>å¿ƒç†&nbsp;&nbsp;&nbsp;<br>è¯­è¨€&nbsp;&nbsp;&nbsp;<br>å­¦ä¼š&nbsp;&nbsp;&nbsp;<br>åœ¨&nbsp;&nbsp;&nbsp;&nbsp;<br>å¹¿å·&nbsp;&nbsp;&nbsp;<br>å¬å¼€&nbsp;&nbsp;&nbsp;<br>ã€‚&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">SRL&nbsp;PA1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>â—„â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â—„â”€â”´â–ºARGM-TMP&nbsp;<br>â—„â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;â”œâ–ºARG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â—„â”€â”˜&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â—„â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â—„â”€â”´â–ºARGM-LOC&nbsp;<br>â•Ÿâ”€â”€â–ºPRED&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Toke&nbsp;<br>â”€â”€â”€â”€&nbsp;<br>2023&nbsp;<br>å¹´&nbsp;&nbsp;&nbsp;&nbsp;<br>å¿ƒç†&nbsp;&nbsp;&nbsp;<br>è¯­è¨€&nbsp;&nbsp;&nbsp;<br>å­¦ä¼š&nbsp;&nbsp;&nbsp;<br>åœ¨&nbsp;&nbsp;&nbsp;&nbsp;<br>å¹¿å·&nbsp;&nbsp;&nbsp;<br>å¬å¼€&nbsp;&nbsp;&nbsp;<br>ã€‚&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>NTâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>M&nbsp;â”€â”€â”€â–ºCLP&nbsp;â”€â”€â”´â–ºQP&nbsp;â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>NNâ”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºNP&nbsp;â”€â”€â”€â”&nbsp;&nbsp;&nbsp;<br>NN&nbsp;&nbsp;â”œâ”€â”€â”€â”€â”€â”€â”€â”€â–ºNP&nbsp;â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”‚&nbsp;&nbsp;&nbsp;<br>NNâ”€â”€â”˜&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”‚&nbsp;&nbsp;&nbsp;<br>P&nbsp;â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºIP<br>NRâ”€â”€â”€â–ºNP&nbsp;â”€â”€â”€â”´â–ºPP&nbsp;â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”‚&nbsp;&nbsp;&nbsp;<br>VVâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºVP&nbsp;â”€â”€â”€â”´â–ºVPâ”€â”€â”€â”€â”¤&nbsp;&nbsp;&nbsp;<br>PUâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;</pre></div><br><div style="display: table; padding-bottom: 1rem;"><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Dep&nbsp;Tre&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>â”Œâ”¬â”€â”€â”¬â”€â”€&nbsp;<br>â”‚â”‚&nbsp;&nbsp;â””â”€â–º&nbsp;<br>â”‚â””â”€â–ºâ”Œâ”€â”€&nbsp;<br>â”‚&nbsp;&nbsp;&nbsp;â””â”€â–º&nbsp;<br>â””â”€â”€â”€â”€â”€â–º&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Relat&nbsp;<br>â”€â”€â”€â”€â”€&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Po&nbsp;<br>â”€â”€&nbsp;<br>VV&nbsp;<br>PN&nbsp;<br>VV&nbsp;<br>NN&nbsp;<br>PU&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">SRL&nbsp;PA1&nbsp;&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>â•Ÿâ”€â”€â–ºPRED&nbsp;<br>â”€â”€â”€â–ºARG1&nbsp;<br>â—„â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â—„â”€â”´â–ºARG2&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">SRL&nbsp;PA2&nbsp;&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>â•Ÿâ”€â”€â–ºPRED&nbsp;<br>â”€â”€â”€â–ºARG1&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Tok&nbsp;<br>â”€â”€â”€&nbsp;<br>æ¬¢è¿&nbsp;&nbsp;<br>å¤§å®¶&nbsp;&nbsp;<br>å‚åŠ &nbsp;&nbsp;<br>å·¥ä½œåŠ&nbsp;<br>ï¼&nbsp;&nbsp;&nbsp;</pre><pre style="display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br>VVâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>PNâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºNPâ”€â”€â”€â”€â”¤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>VVâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºVP&nbsp;â”€â”€â”€â”&nbsp;&nbsp;&nbsp;<br>NNâ”€â”€â”€â–ºNP&nbsp;â”€â”€â”€â”´â–ºVP&nbsp;â”€â”€â”€â”€â–ºIP&nbsp;â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â”œâ–ºIP<br>PUâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&nbsp;&nbsp;&nbsp;</pre></div>



```python
# æå–å‡ºæ¥çš„ç‰¹å¾ç›´æ¥ç´¢å¼•å³å¯
print("å¥å­æ•°é‡ä¸º:", docs.count_sentences())
for i in range(docs.count_sentences()):
    print(docs['tok/fine'][i])
```

    å¥å­æ•°é‡ä¸º: 2
    ['2023', 'å¹´', 'å¿ƒç†', 'è¯­è¨€', 'å­¦ä¼š', 'åœ¨', 'å¹¿å·', 'å¬å¼€', 'ã€‚']
    ['æ¬¢è¿', 'å¤§å®¶', 'å‚åŠ ', 'å·¥ä½œåŠ', 'ï¼']


## 3.0 è¯­è¨€ä»»åŠ¡
åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»¥ä¸»é¢˜åˆ†æä»»åŠ¡å’Œä¸Šä¸‹æ–‡å­¦ä¹ ä¸ºä¾‹ï¼Œæ¼”ç¤ºè¯­è¨€æ¨¡å‹çš„åŠ è½½å’Œæ¨ç†è¿‡ç¨‹ã€‚å¯¹äºå…¶ä»–è¯­è¨€ä»»åŠ¡ï¼Œå‡å¯åœ¨huggingfaceå¹³å°æœç´¢åˆ°ç±»ä¼¼çš„æ•™ç¨‹æ–‡æ¡£ä»¥åŠä»£ç ã€‚
### 3.1 ä¸»é¢˜åˆ†æä»»åŠ¡
ä½¿ç”¨transformersç®¡é“pipelineå¿«é€Ÿå®ç°è¯­è¨€ä»»åŠ¡


```python
# ä»huggingfaceå¹³å°ä¸Šæ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹è·¯å¾„
model_path = 'model/roberta-base-finetuned-chinanews-chinese'

# ä½¿ç”¨transformerså·¥å…·åŒ…åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# åˆ©ç”¨pipelineå¿«é€Ÿè¿›è¡Œè¯­è¨€ä»»åŠ¡
text = 'æ¬¢è¿å‚åŠ å·¥ä½œåŠï¼'
text_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
res = text_classification(text)[0]
print("="*20, "å•ä¸ªå¥å­ä¸»é¢˜åˆ†æè®¡ç®—", "="*20)
print(f"\nInput: {text}\nPrediction: {res['label']}, Score: {res['score']:.3f}")


# pipelineå¯ä»¥å®ç°æ‰¹é‡å¥å­çš„è®¡ç®—
text_lst = ['2023å¹´å¿ƒç†è¯­è¨€å­¦ä¼šåœ¨å¹¿å·å¬å¼€', 'æ¹–äººæœ‰æ„ç­¾ä¿ç½—è¡¥å¼ºï¼Œè”æ‰‹è©¹å§†æ–¯è¿½é€æ€»å† å†›']
res_lst = text_classification(text_lst)
print("\n\n")
print("="*20, "å¤šä¸ªå¥å­æ‰¹é‡è¿›è¡Œä¸»é¢˜åˆ†æè®¡ç®—", "="*20)
for text, res in zip(text_lst, res_lst):
    print(f"\nInput: {text}\nPrediction: {res['label']}, Score: {res['score']:.3f}")
```

    ==================== å•ä¸ªå¥å­ä¸»é¢˜åˆ†æè®¡ç®— ====================
    
    Input: æ¬¢è¿å‚åŠ å·¥ä½œåŠï¼
    Prediction: culture, Score: 0.723
    
    
    
    ==================== å¤šä¸ªå¥å­æ‰¹é‡è¿›è¡Œä¸»é¢˜åˆ†æè®¡ç®— ====================
    
    Input: 2023å¹´å¿ƒç†è¯­è¨€å­¦ä¼šåœ¨å¹¿å·å¬å¼€
    Prediction: culture, Score: 0.969
    
    Input: æ¹–äººæœ‰æ„ç­¾ä¿ç½—è¡¥å¼ºï¼Œè”æ‰‹è©¹å§†æ–¯è¿½é€æ€»å† å†›
    Prediction: sports, Score: 1.000


### 3.2 ä¸Šä¸‹æ–‡å­¦ä¹ 
é€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­ç»™å®šä»»åŠ¡æè¿°å’Œç¤ºä¾‹ï¼Œé€šç”¨çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å¿«é€Ÿå­¦ä¹ è¯­è¨€ä»»åŠ¡ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¸ä½¿ç”¨pipelineï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹æ–¹æ³•è¿›è¡Œè®¡ç®—ã€‚


```python
# ä»huggingfaceå¹³å°ä¸Šæ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹è·¯å¾„
model_path = "model/flan-t5-large"

# ä½¿ç”¨transformerså·¥å…·åŒ…åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)


print("\n\n")
print("="*20, "ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°æ–‡æœ¬ç¿»è¯‘", "="*20)
text = "translate English to German: How old are you?"

# è°ƒç”¨æ¨¡å‹åˆ†è¯å™¨ï¼Œå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„tensorå½¢å¼
input_ids = tokenizer(text, return_tensors="pt").input_ids

# è°ƒç”¨æ¨¡å‹çš„generateæ–¹æ³•
outputs = model.generate(input_ids)
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)
print(f"Input: {text}\nOutput: {decoded_output}")



print("\n\n")
print("="*20, "ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°ä¸»é¢˜æ–‡æœ¬ç”Ÿæˆ", "="*20)
text = '''Generate sentences with the topic : 
sports => Lionel Messi and MLS club Inter Miami are discussing possible signing
entertainment => 
'''

# è°ƒç”¨æ¨¡å‹åˆ†è¯å™¨ï¼Œå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„tensorå½¢å¼
input_ids = tokenizer(text, return_tensors="pt").input_ids

# è°ƒç”¨æ¨¡å‹çš„generateæ–¹æ³•
outputs = model.generate(input_ids)
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)
print(f"Input: {text}\nOutput: {decoded_output}")

```

    
    
    
    ==================== ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°æ–‡æœ¬ç¿»è¯‘ ====================


    /home/zhang/anaconda3/envs/ngram/lib/python3.7/site-packages/transformers/generation/utils.py:1278: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
      UserWarning,


    Input: translate English to German: How old are you?
    Output: Wie alte sind Sie?
    
    
    
    ==================== ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°ä¸»é¢˜æ–‡æœ¬ç”Ÿæˆ ====================
    Input: Generate sentences with the topic : 
    sports => Lionel Messi and MLS club Inter Miami are discussing possible signing
    entertainment => 
    
    Output: a new tv series starring adrian sandler is 


### 3.3 æ–‡æœ¬ç”Ÿæˆè¶…å‚æ•°
åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¼šåˆ†ææ–‡æœ¬ç”Ÿæˆä¸­çš„æ¸©åº¦å‚æ•°ã€æœç´¢ç­–ç•¥å‚æ•°ä»¥åŠtop-på‚æ•°å¯¹æ–‡æœ¬ç”Ÿæˆç»“æœçš„å½±å“ã€‚


```python
# ä»huggingfaceå¹³å°ä¸Šæ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹è·¯å¾„
model_path = "model/flan-t5-large"

# ä½¿ç”¨transformerså·¥å…·åŒ…åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

text = 'Welcome to '

# è°ƒç”¨æ¨¡å‹åˆ†è¯å™¨ï¼Œå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„tensorå½¢å¼
input_ids = tokenizer(text, return_tensors="pt").input_ids

# å…¶ä½™å¯ä¿®æ”¹å‚æ•°åŒ…æ‹¬top_k, top_pç­‰, å¯ç›´æ¥åœ¨.generate()æ–¹æ³•ä¸­è°ƒç”¨
# ref: https://huggingface.co/blog/how-to-generate
print(f'\nInput: {text}\n')
print("="*20, "è´ªå©ªæœç´¢", "="*20)
for iter in range(5):
    outputs = model.generate(input_ids, max_length=10)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)
    print(f"Iter {iter}: {decoded_output}")
    
print("="*20, "éšæœºæœç´¢, æ¸©åº¦å‚æ•°=0.1", "="*20)
for iter in range(5):
    outputs = model.generate(input_ids, do_sample=True, temperature=0.1, max_length=10)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)
    print(f"Iter {iter}: {decoded_output}")
    

print("="*20, "éšæœºæœç´¢, æ¸©åº¦å‚æ•°=1.0", "="*20)
for iter in range(5):
    outputs = model.generate(input_ids, do_sample=True, temperature=1.0, max_length=10)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)
    print(f"Iter {iter}: {decoded_output}")
```

    
    Input: Welcome to 
    
    ==================== è´ªå©ªæœç´¢ ====================
    Iter 0: Welcome to the e-commerce world!
    Iter 1: Welcome to the e-commerce world!
    Iter 2: Welcome to the e-commerce world!
    Iter 3: Welcome to the e-commerce world!
    Iter 4: Welcome to the e-commerce world!
    ==================== éšæœºæœç´¢, æ¸©åº¦å‚æ•°=0.1 ====================
    Iter 0: Welcome to the official website of the Institut
    Iter 1: Welcome to the e-commerce world!
    Iter 2: Welcome to the official website of the 
    Iter 3: Welcome to the official website of the 
    Iter 4: Welcome to the world of e-commerce
    ==================== éšæœºæœç´¢, æ¸©åº¦å‚æ•°=1.0 ====================
    Iter 0: To the Official Fanpage of Xs
    Iter 1: Browse through our collection of christian
    Iter 2: Gozki - the place to be
    Iter 3: Welcome to Cleo Group online-magazin
    Iter 4: Welcome to the world of The Tweeds



```python

```
